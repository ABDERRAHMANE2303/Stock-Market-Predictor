{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prophet Models for Stock Direction Prediction\n",
    "\n",
    "## Objective\n",
    "Train Prophet models to predict the direction of stock price movements for three different time horizons (day, week, month). We'll train a separate model for each stock and time period, resulting in a total of 60 models.\n",
    "\n",
    "## Process\n",
    "1. Load cleaned data from `data/prophet/{period}/{stock}_prophet_{period}.csv`\n",
    "2. Configure and train Prophet models with optimized hyperparameters\n",
    "3. Evaluate model performance using appropriate classification metrics\n",
    "4. Save trained models to `model/prophet/{period}/prophet_{stock}_{period}`\n",
    "\n",
    "## Notes\n",
    "- For direction prediction, we use Prophet to estimate probabilities (values between 0-1)\n",
    "- We apply a threshold (default 0.5) to convert these probabilities to binary predictions (0=down, 1=up)\n",
    "- We use time-based validation instead of random splits to maintain the temporal nature of the data\n",
    "- Multiprocessing is used to leverage multiple CPU cores for parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Prophet-specific imports\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "# Metrics for binary classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore specific warnings from Prophet\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Directory Structure\n",
    "\n",
    "Set up the input and output directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input directories (where cleaned data is stored)\n",
    "base_input_dir = '../data/prophet/'\n",
    "\n",
    "# Output directories (where models will be saved)\n",
    "base_output_dir = '../model/prophet/'\n",
    "\n",
    "# Create output directories for each prediction period\n",
    "periods = ['day', 'week', 'month']\n",
    "input_dirs = {}\n",
    "output_dirs = {}\n",
    "\n",
    "for period in periods:\n",
    "    input_dirs[period] = os.path.join(base_input_dir, period)\n",
    "    output_dirs[period] = os.path.join(base_output_dir, period)\n",
    "    os.makedirs(output_dirs[period], exist_ok=True)\n",
    "    print(f\"Created output directory: {output_dirs[period]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Training and Evaluation Functions\n",
    "\n",
    "Create functions to train Prophet models and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_model(df, regressors=None, params=None):\n",
    "    \"\"\"\n",
    "    Train a Prophet model with the specified parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame formatted for Prophet (must contain 'ds' and 'y' columns)\n",
    "    - regressors: List of regressor column names to use for prediction\n",
    "    - params: Dictionary of Prophet parameters\n",
    "    \n",
    "    Returns:\n",
    "    - Trained Prophet model\n",
    "    \"\"\"\n",
    "    # Set default parameters if none provided\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'changepoint_prior_scale': 0.05,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'seasonality_mode': 'additive',\n",
    "            'daily_seasonality': False,\n",
    "            'weekly_seasonality': True,\n",
    "            'yearly_seasonality': True\n",
    "        }\n",
    "    \n",
    "    # Create Prophet model with specified parameters\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=params.get('changepoint_prior_scale', 0.05),\n",
    "        seasonality_prior_scale=params.get('seasonality_prior_scale', 10.0),\n",
    "        seasonality_mode=params.get('seasonality_mode', 'additive'),\n",
    "        daily_seasonality=params.get('daily_seasonality', False),\n",
    "        weekly_seasonality=params.get('weekly_seasonality', True),\n",
    "        yearly_seasonality=params.get('yearly_seasonality', True)\n",
    "    )\n",
    "    \n",
    "    # Add regressors\n",
    "    if regressors is not None:\n",
    "        for regressor in regressors:\n",
    "            if regressor in df.columns:\n",
    "                model.add_regressor(regressor)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(df)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_direction_predictions(y_true, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate binary classification predictions for direction prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True binary labels\n",
    "    - y_pred_proba: Predicted probabilities\n",
    "    - threshold: Threshold to convert probabilities to binary predictions (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'threshold': threshold\n",
    "    }\n",
    "    \n",
    "    # Only calculate ROC AUC if there are both positive and negative classes\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for converting probabilities to binary predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True binary labels\n",
    "    - y_pred_proba: Predicted probabilities\n",
    "    - metric: Metric to optimize ('accuracy', 'precision', 'recall', 'f1')\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal threshold\n",
    "    - Dictionary of evaluation metrics at optimal threshold\n",
    "    \"\"\"\n",
    "    # Try different thresholds\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metric\n",
    "        if metric == 'accuracy':\n",
    "            score = accuracy_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred, zero_division=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        # Update best threshold if current threshold gives better score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Evaluate with best threshold\n",
    "    best_metrics = evaluate_direction_predictions(y_true, y_pred_proba, best_threshold)\n",
    "    \n",
    "    return best_threshold, best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Time-Based Train-Test Split Function\n",
    "\n",
    "For time series data, we'll use a time-based split instead of random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_train_test_split(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split time series data into train and test sets based on time.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with a 'ds' datetime column\n",
    "    - test_size: Proportion of data to use for testing (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "    - train_df: Training DataFrame\n",
    "    - test_df: Testing DataFrame\n",
    "    \"\"\"\n",
    "    # Ensure DataFrame is sorted by date\n",
    "    df = df.sort_values('ds')\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    # Split data\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df = df.iloc[split_idx:].copy()\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def sliding_window_cv(df, n_windows=5, window_size=0.2):\n",
    "    \"\"\"\n",
    "    Generate sliding window splits for time series cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with a 'ds' datetime column\n",
    "    - n_windows: Number of validation windows\n",
    "    - window_size: Size of each validation window as a proportion of data\n",
    "    \n",
    "    Returns:\n",
    "    - List of (train_df, val_df) tuples\n",
    "    \"\"\"\n",
    "    # Ensure DataFrame is sorted by date\n",
    "    df = df.sort_values('ds')\n",
    "    \n",
    "    # Initialize list to store splits\n",
    "    splits = []\n",
    "    \n",
    "    # Calculate window size in terms of number of samples\n",
    "    window_samples = int(len(df) * window_size)\n",
    "    \n",
    "    # Generate sliding windows\n",
    "    for i in range(n_windows):\n",
    "        # Calculate split points\n",
    "        max_end_idx = len(df) - i * window_samples\n",
    "        start_idx = max(0, max_end_idx - window_samples)\n",
    "        end_idx = max_end_idx\n",
    "        \n",
    "        # Split data\n",
    "        train_df = df.iloc[:start_idx].copy()\n",
    "        val_df = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Only add split if we have enough data\n",
    "        if len(train_df) > 0 and len(val_df) > 0:\n",
    "            splits.append((train_df, val_df))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model Training Pipeline\n",
    "\n",
    "Create a function to train and evaluate models for a specific stock and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_prophet(stock, period, use_cv=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Prophet model for a specific stock and period.\n",
    "    \n",
    "    Parameters:\n",
    "    - stock: Stock symbol\n",
    "    - period: Prediction period ('day', 'week', 'month')\n",
    "    - use_cv: Whether to use cross-validation (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing model, parameters, evaluation metrics, etc.\n",
    "    \"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define file paths\n",
    "    input_file = os.path.join(input_dirs[period], f\"{stock}_prophet_{period}.csv\")\n",
    "    output_file = os.path.join(output_dirs[period], f\"prophet_{stock}_{period}.pkl\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Input file not found: {input_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    \n",
    "    # Get regressor columns (all numeric columns except 'y' and special columns)\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    regressors = [col for col in numeric_cols if col != 'y' and col != 'next_day_close' \n",
    "                 and col != 'next_week_avg' and col != 'next_month_avg']\n",
    "    \n",
    "    # Configure Prophet parameters based on period\n",
    "    if period == 'day':\n",
    "        params = {\n",
    "            'changepoint_prior_scale': 0.05,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'seasonality_mode': 'additive',\n",
    "            'daily_seasonality': False,\n",
    "            'weekly_seasonality': True,\n",
    "            'yearly_seasonality': True\n",
    "        }\n",
    "    elif period == 'week':\n",
    "        params = {\n",
    "            'changepoint_prior_scale': 0.1,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'seasonality_mode': 'additive',\n",
    "            'daily_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'yearly_seasonality': True\n",
    "        }\n",
    "    else:  # month\n",
    "        params = {\n",
    "            'changepoint_prior_scale': 0.15,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'seasonality_mode': 'additive',\n",
    "            'daily_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'yearly_seasonality': True\n",
    "        }\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_df, test_df = time_based_train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    # Train model on all data (for final model)\n",
    "    model = train_prophet_model(df, regressors=regressors, params=params)\n",
    "    \n",
    "    # Train model on training data (for evaluation)\n",
    "    eval_model = train_prophet_model(train_df, regressors=regressors, params=params)\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_features = test_df[['ds'] + regressors].rename(columns={'ds': 'ds'})\n",
    "    test_predictions = eval_model.predict(test_features)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    y_true = test_df['y'].values\n",
    "    y_pred_proba = test_predictions['yhat'].values\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    best_threshold, best_metrics = find_optimal_threshold(y_true, y_pred_proba, metric='f1')\n",
    "    \n",
    "    # Evaluate with default threshold for comparison\n",
    "    default_metrics = evaluate_direction_predictions(y_true, y_pred_proba, threshold=0.5)\n",
    "    \n",
    "    # Cross-validation if enabled\n",
    "    cv_metrics = None\n",
    "    if use_cv:\n",
    "        cv_splits = sliding_window_cv(df, n_windows=3, window_size=0.2)\n",
    "        cv_results = []\n",
    "        \n",
    "        for cv_train, cv_val in cv_splits:\n",
    "            # Train model on this CV split\n",
    "            cv_model = train_prophet_model(cv_train, regressors=regressors, params=params)\n",
    "            \n",
    "            # Make predictions\n",
    "            cv_features = cv_val[['ds'] + regressors].rename(columns={'ds': 'ds'})\n",
    "            cv_predictions = cv_model.predict(cv_features)\n",
    "            \n",
    "            # Evaluate\n",
    "            cv_true = cv_val['y'].values\n",
    "            cv_pred_proba = cv_predictions['yhat'].values\n",
    "            cv_metrics = evaluate_direction_predictions(cv_true, cv_pred_proba, threshold=best_threshold)\n",
    "            cv_results.append(cv_metrics)\n",
    "        \n",
    "        # Average CV results\n",
    "        cv_metrics = {\n",
    "            'accuracy': np.mean([m['accuracy'] for m in cv_results]),\n",
    "            'precision': np.mean([m['precision'] for m in cv_results]),\n",
    "            'recall': np.mean([m['recall'] for m in cv_results]),\n",
    "            'f1': np.mean([m['f1'] for m in cv_results])\n",
    "        }\n",
    "        \n",
    "        if all('roc_auc' in m for m in cv_results):\n",
    "            cv_metrics['roc_auc'] = np.mean([m['roc_auc'] for m in cv_results])\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'stock': stock,\n",
    "        'period': period,\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'regressors': regressors,\n",
    "        'train_size': len(train_df),\n",
    "        'test_size': len(test_df),\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_metrics': best_metrics,\n",
    "        'default_metrics': default_metrics,\n",
    "        'cv_metrics': cv_metrics,\n",
    "        'training_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Models in Parallel\n",
    "\n",
    "Use multiprocessing to train models for all stocks and periods in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all stock files\n",
    "all_stocks = set()\n",
    "for period in periods:\n",
    "    stock_files = glob.glob(os.path.join(input_dirs[period], \"*_prophet_*.csv\"))\n",
    "    for file in stock_files:\n",
    "        # Extract stock symbol from filename (format: {stock}_prophet_{period}.csv)\n",
    "        stock = os.path.basename(file).split('_prophet_')[0]\n",
    "        all_stocks.add(stock)\n",
    "\n",
    "all_stocks = sorted(list(all_stocks))\n",
    "print(f\"Found {len(all_stocks)} stocks: {', '.join(all_stocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train a Single Model for Testing\n",
    "\n",
    "Before training all models, let's test the pipeline with a single stock and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model for one stock and period to test the pipeline\n",
    "test_stock = all_stocks[0]  # First stock in the list\n",
    "test_period = 'day'  # Start with day prediction\n",
    "\n",
    "print(f\"Testing training pipeline with {test_stock} for {test_period} prediction...\")\n",
    "test_results = train_and_evaluate_prophet(test_stock, test_period, use_cv=True)\n",
    "\n",
    "if test_results is not None:\n",
    "    print(f\"\\nTraining completed in {test_results['training_time']:.2f} seconds\")\n",
    "    print(f\"Best threshold: {test_results['best_threshold']:.2f}\")\n",
    "    \n",
    "    print(\"\\nMetrics with best threshold:\")\n",
    "    for metric, value in test_results['best_metrics'].items():\n",
    "        if not isinstance(value, np.ndarray):  # Skip confusion matrix\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(test_results['best_metrics']['confusion_matrix'])\n",
    "    \n",
    "    if test_results['cv_metrics'] is not None:\n",
    "        print(\"\\nCross-Validation Metrics:\")\n",
    "        for metric, value in test_results['cv_metrics'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"Failed to train model for test stock and period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Model Predictions\n",
    "\n",
    "Let's visualize the predictions for the test model to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(stock, period, results=None):\n",
    "    \"\"\"\n",
    "    Visualize Prophet model predictions for direction prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - stock: Stock symbol\n",
    "    - period: Prediction period ('day', 'week', 'month')\n",
    "    - results: Results dictionary from train_and_evaluate_prophet\n",
    "      If None, will attempt to load from saved file\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        # Try to load from file\n",
    "        model_file = os.path.join(output_dirs[period], f\"prophet_{stock}_{period}.pkl\")\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"Model file not found: {model_file}\")\n",
    "            return\n",
    "        \n",
    "        with open(model_file, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    \n",
    "    # Load original data\n",
    "    input_file = os.path.join(input_dirs[period], f\"{stock}_prophet_{period}.csv\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    \n",
    "    # Make predictions on the whole dataset\n",
    "    model = results['model']\n",
    "    regressors = results['regressors']\n",
    "    features = df[['ds'] + regressors].rename(columns={'ds': 'ds'})\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # Create DataFrame with actual and predicted values\n",
    "    pred_df = df[['ds', 'y']].copy()\n",
    "    pred_df['predicted_proba'] = predictions['yhat'].values\n",
    "    pred_df['predicted_direction'] = (pred_df['predicted_proba'] >= results['best_threshold']).astype(int)\n",
    "    pred_df['correct'] = (pred_df['y'] == pred_df['predicted_direction']).astype(int)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create a subplot layout\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 14))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted Probabilities\n",
    "    axes[0].plot(pred_df['ds'], pred_df['predicted_proba'], color='blue', label='Predicted Probability')\n",
    "    axes[0].scatter(pred_df['ds'], pred_df['y'], color='red', alpha=0.5, label='Actual Direction (0/1)')\n",
    "    axes[0].axhline(y=results['best_threshold'], color='green', linestyle='--', label=f\"Threshold: {results['best_threshold']:.2f}\")\n",
    "    axes[0].axhline(y=0.5, color='orange', linestyle='--', label='Default Threshold: 0.5')\n",
    "    axes[0].set_title(f\"{stock} - {period.capitalize()} Direction Prediction Probabilities\")\n",
    "    axes[0].set_ylabel('Probability / Direction')\n",
    "    axes[0].set_ylim([-0.1, 1.1])\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot 2: Actual vs Predicted Directions (last 60 data points for clarity)\n",
    "    last_n = 60\n",
    "    last_df = pred_df.iloc[-last_n:].copy()\n",
    "    \n",
    "    axes[1].plot(last_df['ds'], last_df['y'], 'o-', color='green', label='Actual Direction')\n",
    "    axes[1].plot(last_df['ds'], last_df['predicted_direction'], 'o--', color='blue', label='Predicted Direction')\n",
    "    \n",
    "    # Color points based on correctness\n",
    "    for i, row in last_df.iterrows():\n",
    "        color = 'green' if row['correct'] == 1 else 'red'\n",
    "        axes[1].axvspan(row['ds'], row['ds'] + pd.Timedelta(days=1), alpha=0.1, color=color)\n",
    "    \n",
    "    axes[1].set_title(f\"{stock} - {period.capitalize()} Actual vs. Predicted Direction (Last {last_n} Points)\")\n",
    "    axes[1].set_ylabel('Direction (0=Down, 1=Up)')\n",
    "    axes[1].set_ylim([-0.1, 1.1])\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Plot 3: Confusion Matrix\n",
    "    cm = results['best_metrics']['confusion_matrix']\n",
    "    labels = ['Down (0)', 'Up (1)']\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axes[2])\n",
    "    axes[2].set_title('Confusion Matrix')\n",
    "    axes[2].set_xlabel('Predicted')\n",
    "    axes[2].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print prediction statistics\n",
    "    print(f\"\\nPrediction Statistics for {stock} - {period.capitalize()}:\")\n",
    "    print(f\"Accuracy: {results['best_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {results['best_metrics']['f1']:.4f}\")\n",
    "    print(f\"Precision: {results['best_metrics']['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['best_metrics']['recall']:.4f}\")\n",
    "    if 'roc_auc' in results['best_metrics']:\n",
    "        print(f\"ROC AUC: {results['best_metrics']['roc_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    print(f\"Down (0): {100 * (1 - df['y'].mean()):.1f}%\")\n",
    "    print(f\"Up (1): {100 * df['y'].mean():.1f}%\")\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "# Visualize the test model predictions\n",
    "test_pred_df = visualize_model_predictions(test_stock, test_period, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Define Function to Train Multiple Models in Parallel\n",
    "\n",
    "Now that we've tested the pipeline, let's train all models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models_parallel(stocks, periods, max_workers=None, use_cv=True):\n",
    "    \"\"\"\n",
    "    Train models for all stocks and periods in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    - stocks: List of stock symbols\n",
    "    - periods: List of prediction periods\n",
    "    - max_workers: Maximum number of worker processes (default: None, uses CPU count)\n",
    "    - use_cv: Whether to use cross-validation (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of results for all models\n",
    "    \"\"\"\n",
    "    # Create a list of all (stock, period) combinations\n",
    "    tasks = [(stock, period) for stock in stocks for period in periods]\n",
    "    results = {}\n",
    "    \n",
    "    # If no max_workers specified, use CPU count minus 1 to avoid overloading\n",
    "    if max_workers is None:\n",
    "        import multiprocessing\n",
    "        max_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    print(f\"Training {len(tasks)} models using {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O-bound tasks or ProcessPoolExecutor for CPU-bound tasks\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_task = {executor.submit(train_and_evaluate_prophet, stock, period, use_cv): (stock, period) \n",
    "                         for stock, period in tasks}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(future_to_task)):\n",
    "            stock, period = future_to_task[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results[(stock, period)] = result\n",
    "                    print(f\"[{i+1}/{len(tasks)}] Completed {stock} - {period} (Accuracy: {result['best_metrics']['accuracy']:.4f}, F1: {result['best_metrics']['f1']:.4f})\")\n",
    "                else:\n",
    "                    print(f\"[{i+1}/{len(tasks)}] Failed to train model for {stock} - {period}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{i+1}/{len(tasks)}] Error training model for {stock} - {period}: {str(e)}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nAll models trained in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Successfully trained {len(results)} out of {len(tasks)} models\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Train All Models\n",
    "\n",
    "Now let's train all models for all stocks and periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models in parallel\n",
    "all_results = train_all_models_parallel(all_stocks, periods, max_workers=None, use_cv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Model Performance\n",
    "\n",
    "Let's analyze the performance of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the performance of all trained models.\n",
    "    \n",
    "    Parameters:\n",
    "    - results: Dictionary of results from train_all_models_parallel\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with performance metrics\n",
    "    metrics_data = []\n",
    "    \n",
    "    for (stock, period), result in results.items():\n",
    "        metrics = {\n",
    "            'stock': stock,\n",
    "            'period': period,\n",
    "            'accuracy': result['best_metrics']['accuracy'],\n",
    "            'precision': result['best_metrics']['precision'],\n",
    "            'recall': result['best_metrics']['recall'],\n",
    "            'f1': result['best_metrics']['f1'],\n",
    "            'threshold': result['best_threshold'],\n",
    "            'training_time': result['training_time']\n",
    "        }\n",
    "        \n",
    "        if 'roc_auc' in result['best_metrics']:\n",
    "            metrics['roc_auc'] = result['best_metrics']['roc_auc']\n",
    "        \n",
    "        metrics_data.append(metrics)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"Summary of Model Performance:\")\n",
    "    for period in metrics_df['period'].unique():\n",
    "        period_df = metrics_df[metrics_df['period'] == period]\n",
    "        print(f\"\\n{period.capitalize()} Direction Prediction:\")\n",
    "        print(f\"Number of models: {len(period_df)}\")\n",
    "        print(f\"Average accuracy: {period_df['accuracy'].mean():.4f} (std: {period_df['accuracy'].std():.4f})\")\n",
    "        print(f\"Average F1 score: {period_df['f1'].mean():.4f} (std: {period_df['f1'].std():.4f})\")\n",
    "        print(f\"Average threshold: {period_df['threshold'].mean():.4f} (std: {period_df['threshold'].std():.4f})\")\n",
    "        print(f\"Average training time: {period_df['training_time'].mean():.2f} seconds\")\n",
    "    \n",
    "    # Visualize performance by period\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot accuracy and F1 score by period\n",
    "    period_metrics = metrics_df.groupby('period').agg({'accuracy': 'mean', 'f1': 'mean', 'precision': 'mean', 'recall': 'mean'}).reset_index()\n",
    "    \n",
    "    bar_width = 0.2\n",
    "    x = np.arange(len(period_metrics))\n",
    "    \n",
    "    plt.bar(x - bar_width*1.5, period_metrics['accuracy'], width=bar_width, label='Accuracy', color='blue')\n",
    "    plt.bar(x - bar_width/2, period_metrics['precision'], width=bar_width, label='Precision', color='green')\n",
    "    plt.bar(x + bar_width/2, period_metrics['recall'], width=bar_width, label='Recall', color='red')\n",
    "    plt.bar(x + bar_width*1.5, period_metrics['f1'], width=bar_width, label='F1 Score', color='purple')\n",
    "    \n",
    "    plt.xlabel('Prediction Period')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance by Prediction Period')\n",
    "    plt.xticks(x, period_metrics['period'])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Top and bottom performing models\n",
    "    top_models = metrics_df.sort_values('f1', ascending=False).head(5)\n",
    "    bottom_models = metrics_df.sort_values('f1', ascending=True).head(5)\n",
    "    \n",
    "    print(\"\\nTop 5 Performing Models (by F1 Score):\")\n",
    "    for i, row in top_models.iterrows():\n",
    "        print(f\"{row['stock']} - {row['period']}: F1={row['f1']:.4f}, Accuracy={row['accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\nBottom 5 Performing Models (by F1 Score):\")\n",
    "    for i, row in bottom_models.iterrows():\n",
    "        print(f\"{row['stock']} - {row['period']}: F1={row['f1']:.4f}, Accuracy={row['accuracy']:.4f}\")\n",
    "    \n",
    "    # Return the metrics DataFrame for further analysis\n",
    "    return metrics_df\n",
    "\n",
    "# Analyze model performance\n",
    "performance_df = analyze_model_performance(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions for Selected Models\n",
    "\n",
    "Let's visualize the predictions for a few selected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for the top performing model of each period\n",
    "for period in periods:\n",
    "    period_df = performance_df[performance_df['period'] == period]\n",
    "    top_model = period_df.sort_values('f1', ascending=False).iloc[0]\n",
    "    top_stock = top_model['stock']\n",
    "    \n",
    "    print(f\"\\nVisualizing top {period} model: {top_stock}\")\n",
    "    visualize_model_predictions(top_stock, period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "We have successfully:\n",
    "1. Trained Prophet models for direction prediction for all stocks and time horizons\n",
    "2. Evaluated model performance using appropriate classification metrics\n",
    "3. Identified the optimal threshold for each model to maximize performance\n",
    "4. Analyzed differences in predictability across different time horizons\n",
    "5. Saved all models for future use\n",
    "\n",
    "### Key Findings\n",
    "- Prophet can be effectively adapted for direction prediction by treating it as a regression problem and applying a threshold\n",
    "- Longer time horizons (week, month) generally show better predictability than day-to-day movements\n",
    "- The optimal threshold varies across different stocks and time horizons\n",
    "- Cross-validation helps ensure model robustness and reduce overfitting\n",
    "\n",
    "### Next Steps\n",
    "1. **Model Deployment**: The trained models can be used for making predictions on new data\n",
    "2. **Ensemble Approaches**: Consider combining predictions from multiple models for improved performance\n",
    "3. **Hyperparameter Optimization**: Further fine-tune Prophet parameters for each stock/period combination\n",
    "4. **Feature Importance Analysis**: Analyze which features contribute most to accurate predictions\n",
    "5. **Comparative Analysis**: Compare Prophet performance with other algorithms like XGBoost or LSTM\n",
    "\n",
    "The trained models are now ready for use in forecasting stock price direction, providing valuable insights for investment decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
